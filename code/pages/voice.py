import streamlit as st
import pyttsx3  # å¼•å…¥ pyttsx3
import requests  # å¯¼å…¥ requests
import re  # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—
import json
# åˆå§‹åŒ– pyttsx3 å¼•æ“
engine = pyttsx3.init()

# è®¾ç½®è¯­é€Ÿã€éŸ³é‡ç­‰å±æ€§ï¼ˆå¯é€‰ï¼‰
engine.setProperty('rate', 150)  # è®¾ç½®è¯­é€Ÿ
engine.setProperty('volume', 1)  # è®¾ç½®éŸ³é‡ï¼ˆ0.0 åˆ° 1.0ï¼‰

# è®¾ç½® Ollama API åœ°å€
OLLAMA_API_URL = "http://localhost:11434/api/generate"

st.title("Ollama Chatbot ğŸ¤–")
st.write("ä¸€ä¸ªå¯ä»¥è®°ä½ 20 è½®å¯¹è¯çš„ AI ï¼")

# åˆå§‹åŒ– session_state ç”¨äºå­˜å‚¨èŠå¤©è®°å½•ï¼Œæœ€å¤šä¿ç•™ 20 è½®
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# **å…ˆæ¸²æŸ“å·²å­˜çš„èŠå¤©è®°å½•ï¼Œä¿è¯å³æ—¶æ˜¾ç¤º**
for chat in st.session_state.chat_history:
    with st.chat_message(chat["role"]):
        st.markdown(chat["content"])

# ç”¨æˆ·è¾“å…¥æ¡†
user_input = st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜...")

if user_input:
    # **ç«‹å³æ˜¾ç¤ºç”¨æˆ·è¾“å…¥**
    st.session_state.chat_history.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # **æ‹¼æ¥å†å²å¯¹è¯ï¼Œå½¢æˆä¸Šä¸‹æ–‡**
    formatted_history = "\n".join(
        [f"{chat['role'].capitalize()}: {chat['content']}" for chat in st.session_state.chat_history]
    )

    # **è¯·æ±‚ Ollamaï¼Œå¼€å¯æµå¼å“åº”**
    payload = {
        "model": "deepseek-r1",  # Ollama æ¨¡å‹
        "prompt": f"ä»¥ä¸‹æ˜¯ä½ å’Œç”¨æˆ·çš„å¯¹è¯å†å²ï¼Œè¯·åŸºäºè¿™äº›å†…å®¹å›ç­”é—®é¢˜ï¼š\n{formatted_history}\nAssistant:",
        "stream": True  # å¼€å¯æµå¼è¾“å‡º
    }

    with st.chat_message("assistant"):
        message_placeholder = st.empty()  # **åˆ›å»ºä¸€ä¸ªå ä½ç¬¦ï¼Œç”¨äºåŠ¨æ€æ›´æ–°å†…å®¹**
        full_response = ""

        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)

        if response.status_code == 200:
            for chunk in response.iter_lines():
                if chunk:
                    try:
                        decoded_chunk = chunk.decode("utf-8")  # è§£ç æ•°æ®
                        json_chunk = json.loads(decoded_chunk)  # è§£æ JSON å“åº”

                        response_text = json_chunk.get('response', '').replace("\n", " ").strip()
                        if response_text:
                            full_response += response_text  # é€æ­¥ç´¯ç§¯å“åº”
                            message_placeholder.markdown(full_response + "â–Œ")  # **å®æ—¶æ›´æ–°æ–‡æœ¬**

                    except json.JSONDecodeError:
                        st.warning("æ”¶åˆ°æ— æ•ˆçš„ JSON æ•°æ®ï¼Œè·³è¿‡è¯¥éƒ¨åˆ†å“åº”ã€‚")

            # **æ¸…ç†æ‰ <think> å’Œ </think> ä¹‹é—´çš„å†…å®¹**
            cleaned_response = re.sub(r'<think>.*?</think>', '', full_response)
            message_placeholder.markdown(full_response)  # **å»æ‰å…‰æ ‡**

            # **å­˜å…¥èŠå¤©è®°å½•**
            st.session_state.chat_history.append({"role": "assistant", "content": full_response})

            # **æœ€å¤šä¿ç•™ 20 è½®**
            if len(st.session_state.chat_history) > 20:
                st.session_state.chat_history.pop(0)
            # **æ¸…ç†åçš„æ–‡æœ¬ç”¨äºè¯­éŸ³æ’­æŠ¥**
            engine.say(cleaned_response)
            engine.runAndWait()

        else:
            st.error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
