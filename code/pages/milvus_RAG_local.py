import streamlit as st
import pyttsx3
import requests
import json
import threading
from sentence_transformers import SentenceTransformer
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
from streamlit.components.v1 import html
from audio_recorder_streamlit import audio_recorder
from faster_whisper import WhisperModel
import io
import numpy as np
import re
import time
import streamlit.components.v1 as components
import edge_tts
import sounddevice as sd
import soundfile as sf
import asyncio
import io
import threading
import fitz  # PyMuPDF
import atexit
import docx
# ------------------ é¡µé¢è®¾ç½® ------------------ #
st.set_page_config(
    page_title="DeepSeek + Ollama æœ¬åœ°é—®ç­”æœºå™¨äºº",
    page_icon="ğŸ¤–",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# ------------------ CSSæ ·å¼ ------------------ #
st.markdown("""
<style>
    /* ä¸»å®¹å™¨æ ·å¼ */
    .stApp {
        background-color: #f5f7fa;
    }

    /* èŠå¤©æ¶ˆæ¯æ ·å¼ */
    .stChatMessage {
        border-radius: 15px;
        padding: 12px 18px;
        margin: 8px 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }

    /* ç”¨æˆ·æ¶ˆæ¯æ ·å¼ */
    [data-testid="stChatMessage-user"] {
        background-color: #e3f2fd;
        margin-left: 20%;
    }

    /* åŠ©æ‰‹æ¶ˆæ¯æ ·å¼ */
    [data-testid="stChatMessage-assistant"] {
        background-color: #ffffff;
        margin-right: 20%;
    }

    /* æŒ‰é’®æ ·å¼ */
    .stButton>button {
        border-radius: 20px;
        padding: 8px 16px;
        background-color: #4a6fa5;
        color: white;
        border: none;
        transition: all 0.3s;
    }

    .stButton>button:hover {
        background-color: #3a5a80;
        transform: scale(1.05);
    }

    /* è¯­éŸ³æŒ‰é’®ç‰¹æ®Šæ ·å¼ */
    #voice_button_side {
        background-color: #ff6b6b;
    }

    #voice_button_side:hover {
        background-color: #ff5252;
    }

    /* è¾“å…¥æ¡†æ ·å¼ */
    .stTextInput>div>div>input {
        border-radius: 20px;
        padding: 10px 15px;
    }

    /* çŠ¶æ€æç¤ºæ ·å¼ */
    .status-box {
        background-color: #f8f9fa;
        border-radius: 10px;
        padding: 10px 15px;
        margin: 10px 0;
        border-left: 4px solid #4a6fa5;
    }
</style>
""", unsafe_allow_html=True)

# ------------------ åˆå§‹åŒ– ------------------ #
# ------------------ æ·»åŠ  Live2D çœ‹æ¿å¨˜ ------------------ #
live2d_js = """
<script src="https://fastly.jsdelivr.net/gh/mtcm-0314/live2d-widget@latest/dist/autoload.js"></script>
"""
html(live2d_js, height=400)



stop_speaking = threading.Event()
speak_thread = None


# ------------------ è¯­éŸ³è¯†åˆ«æ¨¡å‹ ------------------ #
@st.cache_resource
def load_whisper_model():
    return WhisperModel("small", device="cpu", compute_type="int8")


whisper_model = load_whisper_model()


# ------------------ è¯­éŸ³åŠŸèƒ½ ------------------ #
stop_speaking = threading.Event()
speak_thread = None

# Edge-TTS æ”¯æŒçš„ä¸­æ–‡è¯­éŸ³é€‰é¡¹
voice_options = {
    "å¥³å£°ï¼ˆæ™“æ™“ï¼‰": "zh-CN-XiaoxiaoNeural",
    "ç”·å£°ï¼ˆäº‘å¸Œï¼‰": "zh-CN-YunxiNeural",
    "ç«¥å£°ï¼ˆäº‘å¤ï¼‰": "zh-CN-YunxiaNeural",
    "æƒ…æ„Ÿ (å°æ¯…ï¼‰": "zh-CN-XiaoyiNeural",
    "æ­£å¼ï¼ˆäº‘æ¨ï¼‰":"zh-CN-YunyangNeural"
}

voice_name = st.selectbox("é€‰æ‹©è¯­éŸ³è§’è‰²", list(voice_options.keys()), index=0)
voice = voice_options[voice_name]

def stop_tts():
    global speak_thread
    stop_speaking.set()
    sd.stop()
    if speak_thread and speak_thread.is_alive():
        speak_thread.join()
    speak_thread = None

def speak(text, voice=voice):
    global speak_thread
    stop_speaking.clear()

    async def run_tts():
        try:
            communicate = edge_tts.Communicate(text, voice)
            audio_stream = communicate.stream()
            audio_bytes = b""
            async for chunk in audio_stream:
                if chunk["type"] == "audio":
                    audio_bytes += chunk["data"]
            audio_array, samplerate = sf.read(io.BytesIO(audio_bytes), dtype="float32")
            if not stop_speaking.is_set():
                sd.play(audio_array, samplerate)
                sd.wait()
        except Exception as e:
            st.error(f"âŒ è¯­éŸ³åˆæˆé”™è¯¯: {e}")

    asyncio.run(run_tts())


# ------------------ åµŒå…¥æ¨¡å‹ ------------------ #
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer(r'C:\Users\mtcm\Desktop\ä¸œç›Ÿæ¯\code\local_embedding_model')


embedding_model = load_embedding_model()


def get_embedding(text):
    return embedding_model.encode([text])[0].tolist()


# ------------------ åº”ç”¨æ ‡é¢˜ ------------------ #
st.title("ğŸ¤– DeepSeek + Ollama æœ¬åœ°é—®ç­”æœºå™¨äºº")
st.markdown("""
<script>
window.addEventListener("DOMContentLoaded", function () {
    const waitInput = setInterval(() => {
        const inputBox = parent.document.querySelector('textarea[data-testid="stChatInput"]');
        const waifuTips = parent.document.getElementById('waifu-tips');

        if (inputBox && waifuTips) {
            clearInterval(waitInput);

            inputBox.addEventListener('input', () => {
                const text = inputBox.value.trim();
                if (text !== "") {
                    const thinkingPhrases = ["æ­£åœ¨æ€è€ƒä¸­...", "è®©æˆ‘æƒ³æƒ³...", "ç¨ç­‰ç‰‡åˆ»ï½", "è¿™ä¸ªé—®é¢˜æœ‰ç‚¹éš¾å‘¢...", "è®©æˆ‘ç¿»ç¿»ç¬”è®°..."];
                    const msg = thinkingPhrases[Math.floor(Math.random() * thinkingPhrases.length)];

                    waifuTips.innerHTML = msg;
                    waifuTips.classList.add("waifu-tips-active");
                }
            });

            inputBox.addEventListener('blur', () => {
                waifuTips.innerHTML = '';
                waifuTips.classList.remove("waifu-tips-active");
            });
        }
    }, 500);
});
</script>
""", unsafe_allow_html=True)



# ------------------ èŠå¤©å†å² ------------------ #
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ï¼Œç”¨ä¸­æ–‡ç®€æ´æ¸…æ™°åœ°å›ç­”é—®é¢˜"}]

# æ˜¾ç¤ºèŠå¤©å†å²
for msg in st.session_state.messages:
    if msg["role"] in ["user", "assistant"]:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

# ------------------ Milvus åˆå§‹åŒ– ------------------ #
CLUSTER_ENDPOINT = "https://in03-4bb3b5dc9f774d4.serverless.ali-cn-hangzhou.cloud.zilliz.com.cn"
TOKEN = "358abb9a7807731d57cc"

try:
    connections.connect(alias="default", uri=CLUSTER_ENDPOINT, token=TOKEN)
except Exception as e:
    st.error(f"âŒ è¿æ¥ Milvus å¤±è´¥: {e}")
    st.stop()

collection_name = "guilin_qa_collection"
existing_collections = utility.list_collections()

if collection_name not in existing_collections:
    id_field = FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True)
    text_field = FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=500)
    vector_field = FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384)
    weight_field = FieldSchema(name="weight", dtype=DataType.FLOAT)
    schema = CollectionSchema(
        fields=[id_field, text_field, vector_field, weight_field]
    )
    collection = Collection(name=collection_name, schema=schema)
    index_params = {
        "index_type": "IVF_FLAT",
        "metric_type": "L2",
        "params": {"nlist": 128}
    }
    collection.create_index(field_name="embedding", index_params=index_params)
else:
    collection = Collection(name=collection_name)


# ----------- æ–‡æœ¬æå–å‡½æ•° ----------- #
def extract_text_from_file(file, file_type):
    if file_type == "pdf":
        text = ""
        with fitz.open(stream=file.read(), filetype="pdf") as doc:
            for page in doc:
                text += page.get_text()
        return text

    elif file_type == "docx":
        doc = docx.Document(file)
        return "\n".join([para.text for para in doc.paragraphs])

    elif file_type == "txt":
        return file.read().decode("utf-8")

    else:
        return ""

# ----------- åˆ†å—å‡½æ•° ----------- #
def chunk_text(text, chunk_size=300, overlap=50):
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

# ----------- è‡ªåŠ¨åˆ é™¤ä¸Šä¼ å‘é‡ ----------- #
def delete_uploaded_vectors():
    if "uploaded_vector_ids" in st.session_state:
        try:
            ids = st.session_state.uploaded_vector_ids
            id_expr = f"id in [{','.join(map(str, ids))}]"
            collection.delete(expr=id_expr)
            st.info(f"ğŸ§¹ è‡ªåŠ¨åˆ é™¤äº† {len(ids)} æ¡å‘é‡ã€‚")
        except Exception as e:
            st.warning(f"âŒ åˆ é™¤å‘é‡å¤±è´¥: {e}")

atexit.register(delete_uploaded_vectors)

# ----------- æ–‡ä»¶ä¸Šä¼  UI ----------- #
uploaded_file = st.file_uploader("ğŸ“„ ä¸Šä¼ æ–‡ä»¶ (æ”¯æŒ PDF / Word / TXT)", type=["pdf", "docx", "txt"])
if uploaded_file:
    file_type = uploaded_file.name.split(".")[-1].lower()
    st.success(f"ğŸ“¤ æ–‡ä»¶ {uploaded_file.name} ä¸Šä¼ æˆåŠŸï¼Œç±»å‹ï¼š{file_type.upper()}")

    with st.spinner("ğŸ“„ æ­£åœ¨æå–æ–‡æœ¬..."):
        text = extract_text_from_file(uploaded_file, file_type)

    if text.strip() == "":
        st.warning("âš ï¸ æ–‡ä»¶ä¸­æ²¡æœ‰æå–åˆ°æœ‰æ•ˆæ–‡æœ¬")
    else:
        chunks = chunk_text(text)
        st.info(f"ğŸ“Œ æå–åˆ° {len(chunks)} ä¸ªæ®µè½ï¼Œæ­£åœ¨ç”Ÿæˆå‘é‡å¹¶å†™å…¥æ•°æ®åº“...")

        try:
            vectors = [get_embedding(chunk) for chunk in chunks]
            insert_result = collection.insert([
                chunks,
                vectors,
                [1.0] * len(chunks)
            ])
            inserted_ids = insert_result.primary_keys
            st.session_state.uploaded_vector_ids = inserted_ids
            st.success(f"âœ… æ’å…¥æˆåŠŸï¼š{len(inserted_ids)} æ¡å‘é‡ï¼Œé€€å‡ºåå°†è‡ªåŠ¨åˆ é™¤ã€‚")
        except Exception as e:
            st.error(f"âŒ å‘é‡æ’å…¥å¤±è´¥: {e}")
# ------------------ æ£€ç´¢å‡½æ•° ------------------ #
def search_similar_docs(query_text, top_k=3):
    query_vector = get_embedding(query_text)
    search_params = {"metric_type": "L2", "params": {"nprobe": 10}}

    results = collection.search(
        data=[query_vector],
        anns_field="embedding",
        param=search_params,
        limit=top_k * 2,
        output_fields=["text", "weight"]
    )

    hits = results[0]
    weighted_results = []
    for hit in hits:
        score = hit.score
        text = hit.entity.get("text")
        weight = hit.entity.get("weight")
        weighted_score = score * (1 + np.log(weight + 1))
        weighted_results.append((text, weighted_score))

    weighted_results.sort(key=lambda x: x[1], reverse=True)
    return [result[0] for result in weighted_results[:top_k]]


# ------------------ æœ¬åœ° OLLAMA è®¾ç½® ------------------ #
OLLAMA_API_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "deepseek-r1"

# ------------------ è¾“å…¥åŒºåŸŸ ------------------ #
# åˆ›å»ºä¸¤åˆ—å¸ƒå±€ï¼šè¾“å…¥æ¡†å’Œè¯­éŸ³æŒ‰é’®
input_col, voice_col = st.columns([5, 1])

with input_col:
    # åˆå§‹åŒ–èŠå¤©è¾“å…¥å€¼
    if "chat_input_value" not in st.session_state:
        st.session_state.chat_input_value = ""
    components.html(
        """
        <div id="waifu-streamlit-input-box"></div>
        """,
        height=0,
    )
    # åˆ›å»ºèŠå¤©è¾“å…¥æ¡†
    user_input = st.chat_input(
        "è¯·è¾“å…¥ä½ çš„é—®é¢˜...",
        key="chat_input"
    )

with voice_col:
    # è¯­éŸ³æŒ‰é’®
    if st.button("ğŸ¤", key="voice_button_side", help="ç‚¹å‡»å¼€å§‹è¯­éŸ³è¾“å…¥"):
        st.session_state.recording = True
        st.session_state.voice_input_result = ""
        st.rerun()

# è¯­éŸ³å½•åˆ¶å’Œå¤„ç†
if st.session_state.get("recording", False):
    with st.container():
        st.markdown('<div class="status-box">ğŸ¤ æ­£åœ¨å½•éŸ³... (è¯´è¯åè‡ªåŠ¨åœæ­¢)</div>', unsafe_allow_html=True)
        audio_bytes = audio_recorder(text="", pause_threshold=2.0, key="audio_recorder")

        if audio_bytes:
            st.session_state.recording = False
            with st.spinner("ğŸ” æ­£åœ¨è¯†åˆ«è¯­éŸ³..."):
                recognized_text = transcribe_audio(audio_bytes)
                if recognized_text:
                    st.session_state.chat_input_value = recognized_text
                    st.rerun()

# å¦‚æœä¼šè¯çŠ¶æ€ä¸­æœ‰é¢„è®¾å€¼ï¼Œä½¿ç”¨å®ƒä½œä¸ºç”¨æˆ·è¾“å…¥
if "chat_input_value" in st.session_state and st.session_state.chat_input_value:
    user_input = st.session_state.chat_input_value
    st.session_state.chat_input_value = ""

# ------------------ å¤„ç†ç”¨æˆ·è¾“å…¥ ------------------ #
if user_input:
    stop_tts()
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    try:
        related_docs = search_similar_docs(user_input, top_k=10)
        st.write("related_docs",related_docs)
        knowledge_context = "\n".join(related_docs)

        system_prompt = f"ä½ æ˜¯ä¸€ä¸ªèªæ˜çš„ AI åŠ©æ‰‹ï¼Œè¯·å‚è€ƒä»¥ä¸‹çŸ¥è¯†å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š\n{knowledge_context}"
    except Exception as e:
        st.error(f"âŒ Milvus æ£€ç´¢å¤±è´¥: {e}")
        st.stop()

    context_messages = [{"role": "system", "content": system_prompt}] + st.session_state.messages[-10:]

    full_prompt = ""
    for msg in context_messages:
        role = msg["role"]
        content = msg["content"]
        if role == "user":
            full_prompt += f"ç”¨æˆ·: {content}\n"
        elif role == "assistant":
            full_prompt += f"åŠ©æ‰‹: {content}\n"
        elif role == "system":
            full_prompt += f"{content}\n"

    try:
        payload = {
            "model": MODEL_NAME,
            "prompt": full_prompt,
            "stream": True
        }
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            response = requests.post(OLLAMA_API_URL, json=payload, stream=True, timeout=(10, 30))
            for line in response.iter_lines():
                if line:
                    data = json.loads(line.decode("utf-8"))
                    content = data.get("response", "")
                    full_response += content
                    message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            clean_response = re.sub(r'<think>.*?</think>', '', full_response, flags=re.DOTALL)
            speak_thread = threading.Thread(target=speak, args=(clean_response,))
            speak_thread.start()

    except Exception as e:
        st.error(f"âŒ Ollama æœ¬åœ°æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")

# ------------------ åº•éƒ¨æ§åˆ¶æŒ‰é’® ------------------ #
button_col1, button_col2 = st.columns([1, 1])

with button_col1:
    if st.button("ğŸ›‘ åœæ­¢æœ—è¯»", help="åœæ­¢å½“å‰è¯­éŸ³æœ—è¯»"):
        stop_tts()

with button_col2:
    if st.button("ğŸ§¹ æ¸…é™¤å¯¹è¯", help="æ¸…é™¤æ‰€æœ‰èŠå¤©å†å²"):
        st.session_state.messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ï¼Œç”¨ä¸­æ–‡ç®€æ´æ¸…æ™°åœ°å›ç­”é—®é¢˜"}]
        st.rerun()
